# Geração Automática de Casos de Teste com Inteligência Artificial

Este projeto investiga o uso de ferramentas baseadas em Inteligência Artificial para **geração automática de testes unitários**, comparando seu desempenho em relação aos testes desenvolvidos manualmente por engenheiros de software.

A pesquisa se insere no contexto da Engenharia de Software atual, onde ciclos de entrega mais rápidos exigem testes eficientes, automatizados e com qualidade consistente.

---

## Contexto

Tradicionalmente, a criação de testes unitários depende de desenvolvedores experientes e exige tempo e esforço significativo. Desafios comuns incluem:

- Processo manual e demorado  
- Dependência da experiência de quem desenvolve  
- Cobertura e qualidade variáveis  
- Manutenção constante quando o código muda  

Ferramentas de IA surgiram como alternativa promissora, incluindo:

- ChatGPT  
- GitHub Copilot  
- Diffblue  
- Coverity  

Elas prometem gerar testes automaticamente a partir do código, mas ainda faltam estudos comparativos que validem sua eficácia prática em cenários reais.

---

## Objetivo Geral

Avaliar a **efetividade e eficiência** de ferramentas de Inteligência Artificial na geração automática de testes unitários, comparando-as com testes manuais desenvolvidos por profissionais, considerando métricas de:

- Cobertura  
- Qualidade  
- Detecção de defeitos  
- Tempo de criação  
- Legibilidade e manutenibilidade  

---

## Objetivos Específicos

- **Comparar a cobertura de código** alcançada pelos testes gerados manualmente e automaticamente.  
- **Avaliar a qualidade dos testes**, usando métricas de mutação e capacidade de detectar falhas.  
- **Mensurar a eficiência temporal** entre criação manual e automatizada.  
- **Avaliar a legibilidade, estrutura e boas práticas** dos testes automatizados em comparação com os manuais.  

---

## Autor

**Gabriel Henrique Mota Rodrigues**  
Engenharia de Software – PUC Minas
